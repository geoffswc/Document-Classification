{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this workbook, we'll use two movie reviews, one positive, one negative, to train a machine learning model to asses sentiment. This isn't enough data for a meaningful model, and we should not expect predictable or consistent results. However, it can be much easier to see what's happening with a very small dataset.\n",
    "\n",
    "First, we'll create a couple of reviews. The sentiment list has two categories - 1 for positive, 0 for negative, corresponding to each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['excellent film, excellent acting, well written screenplay, coherent plot',\n",
    "    'mediocre film, unconvincing acting, stilted dialog, incoherent plot']\n",
    "sentiments = [1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'review': reviews, 'sentiment': sentiments})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-Of-Words\n",
    "\n",
    "To create document vectors for each review, we will first need a full list of all the words used in the collection (in this case, the two documents).\n",
    "\n",
    "We'll do this using the CountVectorizer library from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide *all* the rows of the reviews column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...to extract all terms in the entire document collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and create a matrix (in sparse matrix form) showing the frequency of each term in the two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, you'd want to keep term frequency matrices in sparse form (imagine a document collection with tens or hundreds of thousands of terms, where only a few show up in each document!). For our small collection, we can take a look at the full matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe may make it easier to visualize the relanship between the bag of words (the document vocabulary) and the document term vector for each individual record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.DataFrame({'word': vectorizer.get_feature_names(), \n",
    "              'positive': X_train.todense().tolist()[0],\n",
    "              'negative': X_train.todense().tolist()[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an ML model\n",
    "\n",
    "Now that we have a vector for each record, and the classification for that vector (positive or negative), we can use these records and their classification to train a ML model to determine sentiment based on term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a random forest classifier. We provide the document term vectors (X_train), and the classifications (sentiment scores). \n",
    "\n",
    "Note - once you've gotten your data into document term vectors, scikit-learn makes it easy to swap out different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier().fit(X_train, df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances\n",
    "\n",
    "Some algorithms provide valuable analysis on your model, even if you don't use it as a predictive tool. Scikit-learn's random forest implementation will provide information on feature imporance, the impact different terms are having on the overall predictiveness of the model (see the scikit-learn documentation for more info).\n",
    "\n",
    "Note - because our training data set is small, there will be a heavy element of randomness in the feature importances here. In fact, if you run this workbook repeatedly, you should see substantial variance in these numbers. This tends to be smoothed out with much later datasets and a larger number of trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model['feature_importance'] = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "Let's create two new reviews, positive and negative, and see how our model predicts their sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review = \"excellent film, acting was so so by but the plot was well thought out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_review = \"mediocre acting, everything about this was unconvincing, save your money\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as before, we need to transform these sentences into a term frequency vector, using the bag of words we created for our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vector = vectorizer.transform([positive_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_vector = vectorizer.transform([negative_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_vector.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({'word': vectorizer.get_feature_names(), \n",
    "              'positive': positive_vector.todense().tolist()[0],\n",
    "              'negative': negative_vector.todense().tolist()[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn random forest provides two output types - a discrete category (1 or 0), or a probability for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(positive_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(negative_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(positive_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict_proba(negative_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execise 1 - change the training and test phrases\n",
    "# what effect does this have on feature importance and prediction?\n",
    "\n",
    "# exercise 2 - try different ML classification algorithms. \n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.naive_bayes import MultinomialNB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
